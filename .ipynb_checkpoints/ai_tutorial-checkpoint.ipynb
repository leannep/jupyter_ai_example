{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc540b8c-a443-4b40-9a22-5e75389437bd",
   "metadata": {},
   "source": [
    "## Using jupyter ai in a notebook\n",
    "\n",
    "See this tutorial \n",
    "https://www.google.com/search?q=tutorial+on+jupyter+notebook+ai&rlz=1C5CHFA_enUS1092US1092&oq=tutorial+on+jupyter+notebook+ai&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDY4MzJqMGo3qAIIsAIB&sourceid=chrome&ie=UTF-8#fpstate=ive&vld=cid:a89b1af8,vid:Hsu55UcLAxs,st:0\n",
    "\n",
    "You need an API key for this notebook to work and will need some credits to use the open ai key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb373f6-19a6-4cdf-83f7-f915723b8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b62de0f-6ba1-4ce3-a716-c3ec388b5a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai\n"
     ]
    }
   ],
   "source": [
    "# Load the jupyter ai magics extension\n",
    "%load_ext jupyter_ai\n",
    "\n",
    "# To reload \n",
    "# %reload_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "055102f5-d15e-4b9c-9cd0-e564643efd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock:amazon.titan-text-express-v1`</li><li>`bedrock:amazon.titan-text-lite-v1`</li><li>`bedrock:ai21.j2-ultra-v1`</li><li>`bedrock:ai21.j2-mid-v1`</li><li>`bedrock:cohere.command-light-text-v14`</li><li>`bedrock:cohere.command-text-v14`</li><li>`bedrock:cohere.command-r-v1:0`</li><li>`bedrock:cohere.command-r-plus-v1:0`</li><li>`bedrock:meta.llama2-13b-chat-v1`</li><li>`bedrock:meta.llama2-70b-chat-v1`</li><li>`bedrock:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock:mistral.mistral-large-2402-v1:0`</li></ul> |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock-chat:anthropic.claude-v2`</li><li>`bedrock-chat:anthropic.claude-v2:1`</li><li>`bedrock-chat:anthropic.claude-instant-v1`</li><li>`bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-opus-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0`</li></ul> |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`anthropic:claude-v1`</li><li>`anthropic:claude-v1.0`</li><li>`anthropic:claude-v1.2`</li><li>`anthropic:claude-2`</li><li>`anthropic:claude-2.0`</li><li>`anthropic:claude-instant-v1`</li><li>`anthropic:claude-instant-v1.0`</li><li>`anthropic:claude-instant-v1.2`</li></ul> |\n",
       "| `anthropic-chat` | `ANTHROPIC_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`anthropic-chat:claude-2.0`</li><li>`anthropic-chat:claude-2.1`</li><li>`anthropic-chat:claude-instant-1.2`</li><li>`anthropic-chat:claude-3-opus-20240229`</li><li>`anthropic-chat:claude-3-sonnet-20240229`</li><li>`anthropic-chat:claude-3-haiku-20240307`</li><li>`anthropic-chat:claude-3-5-sonnet-20240620`</li></ul> |\n",
       "| `azure-chat-openai` | `AZURE_OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`cohere:command`</li><li>`cohere:command-nightly`</li><li>`cohere:command-light`</li><li>`cohere:command-light-nightly`</li><li>`cohere:command-r-plus`</li><li>`cohere:command-r`</li></ul> |\n",
       "| `gemini` | `GOOGLE_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`gemini:gemini-1.0-pro`</li><li>`gemini:gemini-1.0-pro-001`</li><li>`gemini:gemini-1.0-pro-latest`</li><li>`gemini:gemini-1.0-pro-vision-latest`</li><li>`gemini:gemini-pro`</li><li>`gemini:gemini-pro-vision`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `mistralai` | `MISTRAL_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`mistralai:open-mistral-7b`</li><li>`mistralai:open-mixtral-8x7b`</li><li>`mistralai:open-mixtral-8x22b`</li><li>`mistralai:mistral-small-latest`</li><li>`mistralai:mistral-medium-latest`</li><li>`mistralai:mistral-large-latest`</li><li>`mistralai:codestral-latest`</li></ul> |\n",
       "| `nvidia-chat` | `NVIDIA_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`nvidia-chat:playground_llama2_70b`</li><li>`nvidia-chat:playground_nemotron_steerlm_8b`</li><li>`nvidia-chat:playground_mistral_7b`</li><li>`nvidia-chat:playground_nv_llama2_rlhf_70b`</li><li>`nvidia-chat:playground_llama2_13b`</li><li>`nvidia-chat:playground_steerlm_llama_70b`</li><li>`nvidia-chat:playground_llama2_code_13b`</li><li>`nvidia-chat:playground_yi_34b`</li><li>`nvidia-chat:playground_mixtral_8x7b`</li><li>`nvidia-chat:playground_neva_22b`</li><li>`nvidia-chat:playground_llama2_code_34b`</li></ul> |\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai:babbage-002`</li><li>`openai:davinci-002`</li><li>`openai:gpt-3.5-turbo-instruct`</li></ul> |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai-chat:gpt-3.5-turbo`</li><li>`openai-chat:gpt-3.5-turbo-0125`</li><li>`openai-chat:gpt-3.5-turbo-0301`</li><li>`openai-chat:gpt-3.5-turbo-0613`</li><li>`openai-chat:gpt-3.5-turbo-1106`</li><li>`openai-chat:gpt-3.5-turbo-16k`</li><li>`openai-chat:gpt-3.5-turbo-16k-0613`</li><li>`openai-chat:gpt-4`</li><li>`openai-chat:gpt-4-turbo`</li><li>`openai-chat:gpt-4-turbo-preview`</li><li>`openai-chat:gpt-4-0613`</li><li>`openai-chat:gpt-4-32k`</li><li>`openai-chat:gpt-4-32k-0613`</li><li>`openai-chat:gpt-4-0125-preview`</li><li>`openai-chat:gpt-4-1106-preview`</li><li>`openai-chat:gpt-4o`</li></ul> |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-text-express-v1\n",
       "* bedrock:amazon.titan-text-lite-v1\n",
       "* bedrock:ai21.j2-ultra-v1\n",
       "* bedrock:ai21.j2-mid-v1\n",
       "* bedrock:cohere.command-light-text-v14\n",
       "* bedrock:cohere.command-text-v14\n",
       "* bedrock:cohere.command-r-v1:0\n",
       "* bedrock:cohere.command-r-plus-v1:0\n",
       "* bedrock:meta.llama2-13b-chat-v1\n",
       "* bedrock:meta.llama2-70b-chat-v1\n",
       "* bedrock:meta.llama3-8b-instruct-v1:0\n",
       "* bedrock:meta.llama3-70b-instruct-v1:0\n",
       "* bedrock:mistral.mistral-7b-instruct-v0:2\n",
       "* bedrock:mistral.mixtral-8x7b-instruct-v0:1\n",
       "* bedrock:mistral.mistral-large-2402-v1:0\n",
       "\n",
       "bedrock-chat\n",
       "* bedrock-chat:anthropic.claude-v2\n",
       "* bedrock-chat:anthropic.claude-v2:1\n",
       "* bedrock-chat:anthropic.claude-instant-v1\n",
       "* bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-opus-20240229-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0\n",
       "\n",
       "anthropic\n",
       "Requires environment variable: ANTHROPIC_API_KEY (set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-2\n",
       "* anthropic:claude-2.0\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "* anthropic:claude-instant-v1.2\n",
       "\n",
       "anthropic-chat\n",
       "Requires environment variable: ANTHROPIC_API_KEY (set)\n",
       "* anthropic-chat:claude-2.0\n",
       "* anthropic-chat:claude-2.1\n",
       "* anthropic-chat:claude-instant-1.2\n",
       "* anthropic-chat:claude-3-opus-20240229\n",
       "* anthropic-chat:claude-3-sonnet-20240229\n",
       "* anthropic-chat:claude-3-haiku-20240307\n",
       "* anthropic-chat:claude-3-5-sonnet-20240620\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable: AZURE_OPENAI_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "cohere\n",
       "Requires environment variable: COHERE_API_KEY (not set)\n",
       "* cohere:command\n",
       "* cohere:command-nightly\n",
       "* cohere:command-light\n",
       "* cohere:command-light-nightly\n",
       "* cohere:command-r-plus\n",
       "* cohere:command-r\n",
       "\n",
       "gemini\n",
       "Requires environment variable: GOOGLE_API_KEY (set)\n",
       "* gemini:gemini-1.0-pro\n",
       "* gemini:gemini-1.0-pro-001\n",
       "* gemini:gemini-1.0-pro-latest\n",
       "* gemini:gemini-1.0-pro-vision-latest\n",
       "* gemini:gemini-pro\n",
       "* gemini:gemini-pro-vision\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "mistralai\n",
       "Requires environment variable: MISTRAL_API_KEY (not set)\n",
       "* mistralai:open-mistral-7b\n",
       "* mistralai:open-mixtral-8x7b\n",
       "* mistralai:open-mixtral-8x22b\n",
       "* mistralai:mistral-small-latest\n",
       "* mistralai:mistral-medium-latest\n",
       "* mistralai:mistral-large-latest\n",
       "* mistralai:codestral-latest\n",
       "\n",
       "nvidia-chat\n",
       "Requires environment variable: NVIDIA_API_KEY (not set)\n",
       "* nvidia-chat:playground_llama2_70b\n",
       "* nvidia-chat:playground_nemotron_steerlm_8b\n",
       "* nvidia-chat:playground_mistral_7b\n",
       "* nvidia-chat:playground_nv_llama2_rlhf_70b\n",
       "* nvidia-chat:playground_llama2_13b\n",
       "* nvidia-chat:playground_steerlm_llama_70b\n",
       "* nvidia-chat:playground_llama2_code_13b\n",
       "* nvidia-chat:playground_yi_34b\n",
       "* nvidia-chat:playground_mixtral_8x7b\n",
       "* nvidia-chat:playground_neva_22b\n",
       "* nvidia-chat:playground_llama2_code_34b\n",
       "\n",
       "ollama\n",
       "* See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`.\n",
       "\n",
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:babbage-002\n",
       "* openai:davinci-002\n",
       "* openai:gpt-3.5-turbo-instruct\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0125\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-1106\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-turbo\n",
       "* openai-chat:gpt-4-turbo-preview\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "* openai-chat:gpt-4-0125-preview\n",
       "* openai-chat:gpt-4-1106-preview\n",
       "* openai-chat:gpt-4o\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "togetherai\n",
       "Requires environment variable: TOGETHER_API_KEY (not set)\n",
       "* togetherai:Austism/chronos-hermes-13b\n",
       "* togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
       "* togetherai:EleutherAI/llemma_7b\n",
       "* togetherai:Gryphe/MythoMax-L2-13b\n",
       "* togetherai:Meta-Llama/Llama-Guard-7b\n",
       "* togetherai:Nexusflow/NexusRaven-V2-13B\n",
       "* togetherai:NousResearch/Nous-Capybara-7B-V1p9\n",
       "* togetherai:NousResearch/Nous-Hermes-2-Yi-34B\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-13b\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-70b\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:davinci-002\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the providers and the ai models that are available. There are many, not just ChatGPT\n",
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd722e-083a-4404-91a1-ca6fb4a913f9",
   "metadata": {},
   "source": [
    "To use any of these models, we have to install the package. The RSP may load some in future. For this demo, open-ai, gemini and anthropic claude have been installed when the Docker image was built. Take a look in the requirements.txt file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2acd285-7c4f-4fd8-8686-23c2776cea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/site-packages (1.54.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# If a model is not installed, you can install it with pip install \n",
    "!pip install openai  # See that this is already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094cca8e-8abb-4749-8b91-c4e69c1ee20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to install a key for each provider/model installed \n",
    "# Need to use docker compose     \n",
    "os.environ['OPENAI_API_KEY']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a11a27e-918e-44d6-8beb-fd9829a6ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ANTHROPIC_API_KEY']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70af7757-7339-4d17-b98e-07c58e2913b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did not install Gemini in the Docker file -- let's do tat now \n",
    "#!pip install gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db4a4c8-ae66-421a-b5dd-01107ef77ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now look to the left -- we see that the Gemini models are listed (were they before) \n",
    "# use this to check the format of the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9de5ab30-0d63-475a-89f6-2b64fb5b1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now need to import an key for gemini\n",
    "os.environ['GOOGLE_API_KEY']=''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8e861-5de9-4419-b826-55060e8924a7",
   "metadata": {},
   "source": [
    " If you do not have any credits you will receive this message\n",
    " \n",
    "`RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4412ff1f-e14d-48ac-97c1-302b55912adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a Python implementation of the requested functions in markdown format:\n",
       "\n",
       "```python\n",
       "def lcm(a, b):\n",
       "    \"\"\"\n",
       "    Compute the lowest common multiple of two integers.\n",
       "    \"\"\"\n",
       "    def gcd(x, y):\n",
       "        while y:\n",
       "            x, y = y, x % y\n",
       "        return x\n",
       "    \n",
       "    return abs(a * b) // gcd(a, b)\n",
       "\n",
       "def test_lcm():\n",
       "    \"\"\"\n",
       "    Run 5 test cases for the lcm function.\n",
       "    \"\"\"\n",
       "    test_cases = [\n",
       "        (4, 6),\n",
       "        (12, 18),\n",
       "        (5, 7),\n",
       "        (3, 9),\n",
       "        (10, 15)\n",
       "    ]\n",
       "    \n",
       "    for i, (a, b) in enumerate(test_cases, 1):\n",
       "        result = lcm(a, b)\n",
       "        print(f\"Test case {i}: LCM({a}, {b}) = {result}\")\n",
       "\n",
       "# Run the test cases\n",
       "test_lcm()\n",
       "```\n",
       "\n",
       "This code defines two functions:\n",
       "\n",
       "1. `lcm(a, b)`: Computes the lowest common multiple of two integers `a` and `b`.\n",
       "2. `test_lcm()`: Runs 5 test cases for the `lcm` function.\n",
       "\n",
       "The `lcm` function uses the formula: LCM(a, b) = |a * b| / GCD(a, b), where GCD is the greatest common divisor. The GCD is computed using the Euclidean algorithm.\n",
       "\n",
       "The `test_lcm` function defines 5 test cases and runs them using the `lcm` function, printing the results.\n",
       "\n",
       "To use these functions, you can simply run the script, which will execute the `test_lcm()` function and display the results of the 5 test cases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "claude-3-5-sonnet-20240620",
        "provider_id": "anthropic-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai anthropic-chat:claude-3-5-sonnet-20240620\n",
    "A function that computes that computes the lowest common multiple of two integers and a function tht runs 5 test cases of the lowest common multiple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc76537-a42e-4351-84f8-507022ba0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default output is formatted by markdown. This can bemodifed using arguments to the ai call -f (format) (see list in pulldown)\n",
    "JSON, HTML, math  format exist as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46a38b9d-5a7e-4d07-a75a-f87bfe2df64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "claude-3-5-sonnet-20240620",
        "provider_id": "anthropic-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai anthropic-chat:claude-3-5-sonnet-20240620 -f code\n",
    "A function that computes that computes the lowest common multiple of two integers and a function tht runs 5 test cases of the lowest common multiple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da5fd053-d41d-481d-8371-5f7c51673c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCM of 4 and 6 is: 12\n",
      "LCM of 12 and 18 is: 36\n",
      "LCM of 5 and 7 is: 35\n",
      "LCM of 3 and 9 is: 9\n",
      "LCM of 10 and 15 is: 30\n"
     ]
    }
   ],
   "source": [
    "def lcm(a, b):\n",
    "    def gcd(x, y):\n",
    "        while y:\n",
    "            x, y = y, x % y\n",
    "        return x\n",
    "    \n",
    "    return abs(a * b) // gcd(a, b)\n",
    "\n",
    "def test_lcm():\n",
    "    test_cases = [\n",
    "        (4, 6),\n",
    "        (12, 18),\n",
    "        (5, 7),\n",
    "        (3, 9),\n",
    "        (10, 15)\n",
    "    ]\n",
    "    \n",
    "    for a, b in test_cases:\n",
    "        result = lcm(a, b)\n",
    "        print(f\"LCM of {a} and {b} is: {result}\")\n",
    "\n",
    "test_lcm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dbe4ee4-90ed-4f8d-861f-67dec7c3ad01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial u}{\\partial t} = \\alpha \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "text/latex": {
       "jupyter_ai": {
        "model_id": "claude-3-5-sonnet-20240620",
        "provider_id": "anthropic-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai anthropic-chat:claude-3-5-sonnet-20240620 -f math\n",
    "Generate the 2D heat equationin LaTeX surrounded by `$$`. Do not include an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30c8cf6d-77dd-4f47-817f-1daf279ec8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle w = \\frac{p}{\\rho c^2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "text/latex": {
       "jupyter_ai": {
        "model_id": "claude-3-5-sonnet-20240620",
        "provider_id": "anthropic-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai anthropic-chat:claude-3-5-sonnet-20240620 -f math\n",
    "Generate the cosmic equation of state in LaTex surrounded by `$$`. Do not incude an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "677b487f-424f-446c-9307-29ff07eafa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# work with code located elsewhere. The fillowing cell references the code in this cell\n",
    "for i in range (1,9):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49b5fb83-75ec-491c-b070-35c90033b2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's an explanation of the code in markdown format:\n",
       "\n",
       "```markdown\n",
       "# Code Explanation\n",
       "\n",
       "The code you provided is not a complete Python script, but rather a special command used in Jupyter notebooks or similar interactive environments. Let's break it down:\n",
       "\n",
       "1. `get_ipython().run_cell_magic('ai', 'chatgpt', '...')`:\n",
       "   - This is a method call to execute a \"cell magic\" in an IPython environment.\n",
       "   - Cell magics are special commands that can modify the behavior of entire cells in notebooks.\n",
       "\n",
       "2. `%%ai chatgpt`:\n",
       "   - This is the cell magic being called, likely a custom magic command for interacting with ChatGPT or a similar AI model.\n",
       "\n",
       "3. The string following the magic command:\n",
       "   ```\n",
       "   #A function that computes that computes the lowest common multiple of two integers and a functin tht runs 5 test cases of the lowest common multiple function\n",
       "   ```\n",
       "   - This is a comment describing the task or code to be generated.\n",
       "   - It requests two functions:\n",
       "     1. A function to compute the lowest common multiple (LCM) of two integers.\n",
       "     2. A function to run 5 test cases for the LCM function.\n",
       "\n",
       "This code snippet is not actually implementing these functions, but rather requesting an AI (presumably ChatGPT) to generate the code for these functions.\n",
       "\n",
       "To actually implement these functions, you would need to write Python code for:\n",
       "1. An LCM function\n",
       "2. A test function that calls the LCM function with 5 different test cases\n",
       "\n",
       "The AI is expected to generate this code based on the given description.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "claude-3-5-sonnet-20240620",
        "provider_id": "anthropic-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai anthropic-chat:claude-3-5-sonnet-20240620\n",
    "Explain the code below\n",
    "--\n",
    "{In[21]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3f1577f-a86f-4ec4-81da-29ed908ccc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%ai anthropic-chat:claude-3-5-sonnet-20240620\n",
    "#Explain the error below\n",
    "#--\n",
    "#{Err[21]} \n",
    "\n",
    "# The above might work with openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "323e1e02-ca37-4f8f-a30c-99ecaf597ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also use the left panel to get an explanation of code. Copy the generated code to the left panel. Type :Wat does this code do\" \n",
    "# Select the cell and then use the pull down to choose \"Send message with selection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8edea2b5-121a-4cdc-bbe4-bfabca523acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the left panel to generate a whole notebook with `/generate'\n",
    "# /generate a demo of how to use pandas\n",
    "# A notebook will be generated from scratch. It will take a few minutes \n",
    "# \"Great, I will get started on your notebook. It may take a few minutes, but I will reply here when the notebook is ready. In the meantime, you can continue to ask me other questions.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
